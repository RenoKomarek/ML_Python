{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6291ed19",
   "metadata": {
    "id": "6291ed19"
   },
   "source": [
    "# Assignment 1: Predicting Stroke\n",
    "\n",
    "## 0 Introduction\n",
    "\n",
    "This notebook is part of the stroke prediction competition on Kaggle. Group 3, consisting of the following members, will attempt to build a predictive model for strokes using machine learning techniques:\n",
    "\n",
    "- **Brouwer, Louis**\n",
    "- **Cabraja, Gabrijel**\n",
    "- **Komarek, Reno**\n",
    "- **Sewdien, Akshay**\n",
    "\n",
    "In this notebook, we will follow a structured approach to explore, clean, and model the data in order to predict stroke occurrences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57260420",
   "metadata": {
    "id": "57260420"
   },
   "source": [
    "# Sub-Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89acb53a",
   "metadata": {
    "id": "89acb53a"
   },
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we will explore the data, describe its properties, and identify patterns or issues.\n",
    "\n",
    "### 1.1 Data Overview\n",
    "\n",
    "First, we load the datasets and check the basic structure of the data, such as column names, data types, and missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b018a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "936b018a",
    "outputId": "74242e2f-29d2-4b78-9dbe-f287623db202"
   },
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load train and test datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wDTcNVymcij_",
   "metadata": {
    "id": "wDTcNVymcij_"
   },
   "source": [
    "### 1.2 Explanation of Each Column in the Dataset\n",
    "\n",
    "The dataset used in this project contains information about patients and some of their health-related attributes, which will be used to predict the likelihood of having a stroke. Below is a detailed explanation of each column:\n",
    "\n",
    "1. **id**: Unique identifier for each patient. This column is not relevant for prediction and will be dropped in later steps.\n",
    "   \n",
    "2. **age**: The age of the patient (numerical). This is an important feature as age is often correlated with health conditions such as stroke.\n",
    "   \n",
    "3. **hypertension**: A binary variable indicating whether the patient has hypertension (1 = Yes, 0 = No). This is an important predictor of strokes.\n",
    "   \n",
    "4. **heart_disease**: A binary variable indicating whether the patient has heart disease (1 = Yes, 0 = No). Heart disease is a known risk factor for stroke.\n",
    "   \n",
    "5. **avg_glucose_level**: The average glucose level of the patient (numerical). High glucose levels could be an indicator of diabetes, which increases the risk of stroke.\n",
    "   \n",
    "6. **bmi**: The Body Mass Index (BMI) of the patient (numerical). BMI is a measure of body fat and can be related to stroke risk.\n",
    "   \n",
    "7. **gender_Female**, **gender_Male**, **gender_Other**: These are dummy variables representing the gender of the patient. Since gender is not numeric, it has been converted into three dummy variables.\n",
    "   \n",
    "8. **ever_married_No**, **ever_married_Yes**: These are dummy variables representing the marital status of the patient. Being married might correlate with lifestyle choices that affect health.\n",
    "   \n",
    "9. **work_type_Govt_job**, **work_type_Never_worked**, **work_type_Private**, **work_type_Self-employed**, **work_type_children**: Dummy variables representing the type of employment of the patient. Employment status and type might impact stress levels, access to healthcare, and other factors relevant to stroke risk.\n",
    "   \n",
    "10. **Residence_type_Rural**, **Residence_type_Urban**: These are dummy variables representing the type of residence (rural or urban). Living conditions may affect healthcare access and overall health.\n",
    "   \n",
    "11. **smoking_status_formerly smoked**, **smoking_status_never smoked**, **smoking_status_smokes**: Dummy variables representing the smoking status of the patient. Smoking is a significant risk factor for stroke.\n",
    "   \n",
    "12. **stroke**: This is the target variable (1 = Yes, 0 = No) indicating whether the patient has experienced a stroke. Our goal is to predict this variable based on the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OH0hMyHNdEQM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH0hMyHNdEQM",
    "outputId": "4da891d4-72dd-4efa-909b-d8315bb2720d"
   },
   "outputs": [],
   "source": [
    "# Display the column names\n",
    "print(\"Columns in the dataset:\")\n",
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e096e6",
   "metadata": {
    "id": "45e096e6"
   },
   "source": [
    "### 1.3 Data Types and Basic Statistics\n",
    "\n",
    "To better understand the structure and distribution of our dataset, we will check the data types of each column, check for missing values, and display basic statistics such as mean, standard deviation, min/max values, and quartiles for the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16b8cd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b16b8cd3",
    "outputId": "e585f7bc-7273-47b6-971c-0cfcd4763d85"
   },
   "outputs": [],
   "source": [
    "# Display the data types of each column\n",
    "print(\"Data types of each column:\")\n",
    "print(train_data.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Checking for missing values:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Display basic statistics for numerical columns\n",
    "print(\"\\nBasic statistics of numerical columns:\")\n",
    "print(train_data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fee947",
   "metadata": {
    "id": "32fee947"
   },
   "source": [
    "The table above presents the fundamental statistical characteristics of the numerical columns in our dataset. The following section offers a summary and detailed explanation of each significant feature.\n",
    "\n",
    "1. **id**:\n",
    "   - This column represents a unique identifier for each patient. It does not provide any useful information for predicting strokes and can be dropped later.\n",
    "\n",
    "2. **age**:\n",
    "   - The average age of patients in the dataset is **41.8 years**.\n",
    "   - The minimum age is **0.08 years**, which could be due to newborns or potential outliers.\n",
    "   - The maximum age is **82 years**.\n",
    "   - Most patients are between **24** and **59 years old** (25th to 75th percentile).\n",
    "\n",
    "3. **hypertension**:\n",
    "   - Hypertension is a binary variable indicating whether a patient has high blood pressure.\n",
    "   - About **8.8%** of the patients have hypertension, as shown by the mean value of **0.088**.\n",
    "\n",
    "4. **heart_disease**:\n",
    "   - This is another binary variable indicating if a patient has heart disease.\n",
    "   - The dataset shows that around **4.3%** of the patients have heart disease, based on the mean value of **0.043**.\n",
    "\n",
    "5. **avg_glucose_level**:\n",
    "   - The average glucose level in the dataset is **103.6 mg/dL**.\n",
    "   - The glucose levels range from **55.01 mg/dL** to **281.59 mg/dL**, with most patients having glucose levels between **77.46 mg/dL** and **111.44 mg/dL** (25th to 75th percentile).\n",
    "\n",
    "6. **bmi**:\n",
    "   - The average Body Mass Index (BMI) is **28.6**, which is within the normal to overweight range.\n",
    "   - However, the BMI values range from **10.1** to **97.6**, with the maximum value of **97.6** being extremely high and possibly an outlier.\n",
    "   - Most patients have a BMI between **23.3** and **32.8**.\n",
    "   - This high BMI value will likely need further investigation and could be considered for removal during the data cleaning process.\n",
    "\n",
    "7. **stroke**:\n",
    "   - This is the target variable indicating whether a patient has had a stroke (1 = Yes, 0 = No).\n",
    "   - The mean value of **0.015** suggests that about **1.5%** of the patients in the dataset have had a stroke, which indicates that this is an imbalanced dataset where strokes are relatively rare.\n",
    "\n",
    "#### Note:\n",
    "Some values, such as the **BMI** of **97.6**, fall outside typical ranges and may represent outliers. During the data cleaning process, we will assess whether these extreme values should be removed or corrected to improve the quality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6440ce9",
   "metadata": {
    "id": "b6440ce9"
   },
   "source": [
    "### 1.4 Checking Minimum Requirements for Machine Learning with Scikit-Learn\n",
    "\n",
    "Before applying machine learning algorithms, it is important to ensure that the dataset meets the basic requirements for use with Scikit-Learn.\n",
    "\n",
    "#### 1. **Numerical Features**:\n",
    "   - Machine learning models in Scikit-Learn require numerical features. In our dataset, categorical variables like `gender`, `work_type`, and `smoking_status` have already been converted into dummy variables (binary columns), which ensures that all the data is in a numeric format.\n",
    "   \n",
    "#### 2. **No Missing Values**:\n",
    "   - Scikit-Learn does not handle missing values directly. We checked the dataset for missing values, and there are no missing values present, meaning no further action is required.\n",
    "\n",
    "#### 3. **Data Cleaning**:\n",
    "   - We noticed that some of the dummy variables represent mutually exclusive categories (e.g., `gender_Male` and `gender_Female`). In these cases, one of the columns may be redundant. In the next steps, we will investigate and remove any redundant columns to ensure the dataset is optimized for machine learning.\n",
    "\n",
    "#### 4. **Feature Scaling**:\n",
    "   - Some algorithms (like Logistic Regression, k-NN, and SVM) require that the features are on a similar scale. Features like `age`, `bmi`, and `avg_glucose_level` have different scales, so it might be beneficial to standardize them, especially for models that rely on distance-based metrics (e.g., k-NN).\n",
    "   \n",
    "#### Conclusion:\n",
    "- The dataset is mostly ready for Scikit-Learn algorithms. We will apply feature scaling in the next step if needed, depending on the model choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c74f9",
   "metadata": {
    "id": "625c74f9"
   },
   "source": [
    "### 1.5 Checking and Removing Redundant Dummy Variables\n",
    "\n",
    "In the dataset, some dummy variables might represent mutually exclusive categories (e.g., `ever_married_No` and `ever_married_Yes`). If one of these dummy variables equals 1, the other must necessarily be 0. Such pairs are redundant because they contain the same information. Similarly, we will check other columns to identify and remove redundancy.\n",
    "\n",
    "#### Steps:\n",
    "1. **Mutual exclusivity check**: We will check if the following dummy variable pairs are inversely related:\n",
    "   - `ever_married_No` vs `ever_married_Yes`\n",
    "   - `Residence_type_Rural` vs `Residence_type_Urban`\n",
    "   - `gender_Female` vs `gender_Male`\n",
    "\n",
    "2. **Smoking status columns**: We will check if the smoking status columns (`smoking_status_formerly smoked`, `smoking_status_never smoked`, `smoking_status_smokes`) are mutually exclusive, meaning only one of these columns should be 1 in each row.\n",
    "\n",
    "If any columns are found to be redundant, we will remove them to simplify the dataset and reduce unnecessary complexity for the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd2255",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0fd2255",
    "outputId": "3d70219e-adfd-46c3-da0d-5601a814fcf5"
   },
   "outputs": [],
   "source": [
    "# 1. Check if 'ever_married_No' and 'ever_married_Yes' are inverses of each other\n",
    "ever_married_check = (train_data['ever_married_No'] == (1 - train_data['ever_married_Yes'])).all()\n",
    "\n",
    "# 2. Check if 'Residence_type_Rural' and 'Residence_type_Urban' are inverses of each other\n",
    "residence_type_check = (train_data['Residence_type_Rural'] == (1 - train_data['Residence_type_Urban'])).all()\n",
    "\n",
    "# 3. Check if 'gender_Female' and 'gender_Male' are inverses of each other\n",
    "gender_check = (train_data['gender_Female'] == (1 - train_data['gender_Male'])).all()\n",
    "\n",
    "# 4. Check if smoking status columns ('formerly smoked', 'never smoked', 'smokes') sum to 1, indicating mutual exclusivity\n",
    "smoking_status_sum = train_data[['smoking_status_formerly smoked', 'smoking_status_never smoked', 'smoking_status_smokes']].sum(axis=1)\n",
    "\n",
    "# Display results\n",
    "print(\"Are 'ever_married_No' and 'ever_married_Yes' inverses? ->\", ever_married_check)\n",
    "print(\"Are 'Residence_type_Rural' and 'Residence_type_Urban' inverses? ->\", residence_type_check)\n",
    "print(\"Are 'gender_Female' and 'gender_Male' inverses? ->\", gender_check)\n",
    "print(\"Unique sums for smoking status columns (should be 1 if mutually exclusive):\", smoking_status_sum.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84a54e",
   "metadata": {
    "id": "cb84a54e"
   },
   "source": [
    "The results are as follows:\n",
    "\n",
    "- **`ever_married_No` and `ever_married_Yes`**: These columns are inversely related, as one being 1 implies the other must be 0, and vice versa. Therefore, we can remove one of these columns.\n",
    "- **`Residence_type_Rural` and `Residence_type_Urban`**: These columns are also inversely related. One being 1 means the other is 0, so we can remove one of these columns as well.\n",
    "- **`gender_Female` and `gender_Male`**: These columns are **not** inversely related, meaning both columns provide distinct information and should be kept in the dataset.\n",
    "- **Smoking status columns (`smoking_status_formerly smoked`, `smoking_status_never smoked`, `smoking_status_smokes`)**: These columns are mutually exclusive for most cases, as the sum is either 1 or 0. A sum of 1 means only one smoking status is true, while a sum of 0 likely represents an unknown or unspecified smoking status. Therefore, these columns do not need to be removed.\n",
    "\n",
    "**Conclusion:**\n",
    "- We will remove the redundant columns `ever_married_No` and `Residence_type_Rural` to simplify the dataset. The other columns (`gender_Female`, `gender_Male`, and smoking status columns) will be retained as they provide distinct information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827ebf2",
   "metadata": {
    "id": "2827ebf2"
   },
   "source": [
    "### 1.6 Data Cleaning: Removing Irrelevant and Redundant Columns\n",
    "\n",
    "In this section, we will clean the dataset by removing irrelevant columns, such as the `id` column, and redundant dummy variables that provide the same information as others (see 1.5). We will also filter out rows with unrealistic values in `BMI` and `avg_glucose_level` to ensure the quality of the dataset.\n",
    "\n",
    "#### Steps:\n",
    "1. **Removing Irrelevant Columns**:\n",
    "   - We will remove the `id` column, as it is only used for identification and does not contribute to the prediction of strokes.\n",
    "   - Additionally, we will remove columns that are redundant:\n",
    "     - **`ever_married_No`** is redundant, as its information is inversely related to `ever_married_Yes`. If a person is not married, `ever_married_Yes` will simply be `0`.\n",
    "     - **`Residence_type_Rural`** is redundant, as it is inversely related to `Residence_type_Urban`. If the person does not live in an urban area, `Residence_type_Urban` will be `0`.\n",
    "\n",
    "2. **Handling Unusual Values**:\n",
    "   - We will filter out rows where the `BMI` and `avg_glucose_level` fall outside realistic ranges based on known medical standards:\n",
    "     - **BMI** values below **10** or above **50** are considered extreme and likely to indicate data entry errors. According to the **World Health Organization (WHO)**, a BMI above 40 indicates extreme obesity, but we will allow for slightly higher values to capture realistic cases. A BMI below **10** is too low and unlikely to represent a healthy individual.\n",
    "       - Source: [WHO BMI Classifications](https://www.who.int/health-topics/obesity#tab=tab_1)\n",
    "     - **Glucose levels** below **55 mg/dL** or above **250 mg/dL** are considered unrealistic. Normal fasting glucose levels typically range between **70 and 100 mg/dL**. Values outside this range might indicate severe hypo- or hyperglycemia, but for safety, we allow slightly broader limits.\n",
    "       - Source: [American Diabetes Association](https://www.diabetes.org/diabetes/medication-management/blood-glucose-testing-and-control)\n",
    "\n",
    "By performing this cleaning, we ensure that the dataset is ready for machine learning models and free from unnecessary noise or outliers, which could skew the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189904ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "189904ee",
    "outputId": "037bdc83-ed9d-42fc-fc24-f29caf9fce7a"
   },
   "outputs": [],
   "source": [
    "# Dropping columns holding redundant information\n",
    "train_data_cleaned = train_data.drop(columns=['id', 'ever_married_No', 'Residence_type_Rural'])\n",
    "\n",
    "# Filtering out unrealistic BMI and glucose values\n",
    "train_data_cleaned = train_data_cleaned[\n",
    "    (train_data_cleaned['bmi'] >= 10) & (train_data_cleaned['bmi'] <= 50) &\n",
    "    (train_data_cleaned['avg_glucose_level'] >= 55) & (train_data_cleaned['avg_glucose_level'] <= 250)\n",
    "]\n",
    "\n",
    "# Display the cleaned dataset\n",
    "train_data_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34be8b4",
   "metadata": {
    "id": "f34be8b4"
   },
   "source": [
    "### 1.7 Examining Relationships Between Independent Variables and the Target (Stroke)\n",
    "\n",
    "In this section, we will investigate the relationships between independent variables (such as age, BMI, and glucose levels) and the target variable (`stroke`).\n",
    "\n",
    "We will use the following visualizations to explore these relationships:\n",
    "1. **Histograms** to show the distribution of `age`, `BMI`, and `avg_glucose_level` for stroke and non-stroke groups.\n",
    "2. **Boxplots** to compare the distribution of these variables between patients with and without a stroke.\n",
    "3. **Scatterplots** to examine potential trends or correlations between these continuous variables and the likelihood of a stroke.\n",
    "\n",
    "Let’s take a look at the visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e14baa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "22e14baa",
    "outputId": "5ae492ce-5884-47a5-a7bf-91ee11be7e57"
   },
   "outputs": [],
   "source": [
    "# Load visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for the plots\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Histograms to visualize the distribution of age, BMI, and glucose for stroke and non-stroke groups\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Histogram for Age\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(train_data_cleaned[train_data_cleaned['stroke'] == 0]['age'], color='blue', label='No Stroke', kde=True, bins=30)\n",
    "sns.histplot(train_data_cleaned[train_data_cleaned['stroke'] == 1]['age'], color='red', label='Stroke', kde=True, bins=30)\n",
    "plt.title('Age Distribution for Stroke and Non-Stroke')\n",
    "plt.xlabel('Age')\n",
    "plt.legend()\n",
    "\n",
    "# Histogram for BMI\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(train_data_cleaned[train_data_cleaned['stroke'] == 0]['bmi'], color='blue', label='No Stroke', kde=True, bins=30)\n",
    "sns.histplot(train_data_cleaned[train_data_cleaned['stroke'] == 1]['bmi'], color='red', label='Stroke', kde=True, bins=30)\n",
    "plt.title('BMI Distribution for Stroke and Non-Stroke')\n",
    "plt.xlabel('BMI')\n",
    "plt.legend()\n",
    "\n",
    "# Histogram for Glucose Level\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(train_data_cleaned[train_data_cleaned['stroke'] == 0]['avg_glucose_level'], color='blue', label='No Stroke', kde=True, bins=30)\n",
    "sns.histplot(train_data_cleaned[train_data_cleaned['stroke'] == 1]['avg_glucose_level'], color='red', label='Stroke', kde=True, bins=30)\n",
    "plt.title('Glucose Level Distribution for Stroke and Non-Stroke')\n",
    "plt.xlabel('Average Glucose Level')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GZu7VzkvNa3-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "GZu7VzkvNa3-",
    "outputId": "a0853447-0dee-4812-82e9-9dc2aecff671"
   },
   "outputs": [],
   "source": [
    "# Boxplots to check relationships between variables and stroke\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Boxplot for Age vs Stroke\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x='stroke', y='age', data=train_data_cleaned)\n",
    "plt.title('Age vs Stroke')\n",
    "\n",
    "# Boxplot for BMI vs Stroke\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(x='stroke', y='bmi', data=train_data_cleaned)\n",
    "plt.title('BMI vs Stroke')\n",
    "\n",
    "# Boxplot for Glucose Level vs Stroke\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x='stroke', y='avg_glucose_level', data=train_data_cleaned)\n",
    "plt.title('Glucose Level vs Stroke')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EVhVphsaNh31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "EVhVphsaNh31",
    "outputId": "bc98f119-b063-4f95-da1d-efb23bd644f2"
   },
   "outputs": [],
   "source": [
    "# Scatter plots to visualize any trends\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Scatterplot for Age vs Stroke\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(x='age', y='stroke', data=train_data_cleaned, alpha=0.6)\n",
    "plt.title('Age vs Stroke')\n",
    "\n",
    "# Scatterplot for BMI vs Stroke\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(x='bmi', y='stroke', data=train_data_cleaned, alpha=0.6)\n",
    "plt.title('BMI vs Stroke')\n",
    "\n",
    "# Scatterplot for Glucose Level vs Stroke\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(x='avg_glucose_level', y='stroke', data=train_data_cleaned, alpha=0.6)\n",
    "plt.title('Glucose Level vs Stroke')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76c75b",
   "metadata": {
    "id": "8f76c75b"
   },
   "source": [
    "### Interpretation of the Visualizations:\n",
    "\n",
    "1. **Histograms**:\n",
    "   - The histograms show the distribution of the `age`, `BMI`, and `avg_glucose_level` for both stroke and non-stroke groups.\n",
    "   - We observe that stroke patients tend to be older, with a noticeable shift in the age distribution toward higher values for those who had a stroke.\n",
    "   - The `BMI` and `avg_glucose_level` distributions show that stroke patients might have slightly higher values on average, but the difference is less pronounced compared to age.\n",
    "\n",
    "2. **Boxplots**:\n",
    "   - The boxplots reveal that the median `age` for stroke patients is higher than for non-stroke patients, highlighting a clear trend.\n",
    "   - While there is a slight increase in `BMI` and `avg_glucose_level` for stroke patients, the difference is not as significant as with age.\n",
    "\n",
    "3. **Scatterplots**:\n",
    "   - The scatterplots show potential trends between the independent variables and the target variable (`stroke`).\n",
    "   - For example, we see that older patients are more likely to have had a stroke, whereas the relationships between `BMI` and `glucose levels` with stroke are less clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fabe3",
   "metadata": {
    "id": "9e3fabe3"
   },
   "source": [
    "### 1.8 Dummy Variables Analysis\n",
    "\n",
    "After analyzing the numerical features, we now investigate the relationships between some categorical dummy variables (such as `gender`, `smoking_status`, and `work_type`) and the target variable (`stroke`). These variables have been encoded into binary or categorical values, and we will use bar plots to visualize their potential influence on the likelihood of having a stroke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e3e56",
   "metadata": {
    "id": "a08e3e56",
    "outputId": "227f56ce-0540-4854-d05e-53dad38a9dd9"
   },
   "outputs": [],
   "source": [
    "# Barplot for Gender (Female vs Male) and Stroke\n",
    "sns.countplot(data=train_data_cleaned, x='gender_Male', hue='stroke')\n",
    "plt.title('Gender (Male) vs Stroke')\n",
    "plt.show()\n",
    "\n",
    "# Barplot for Smoking Status and Stroke\n",
    "sns.countplot(data=train_data_cleaned, x='smoking_status_smokes', hue='stroke')\n",
    "plt.title('Smoking Status (Smokes) vs Stroke')\n",
    "plt.show()\n",
    "\n",
    "# Barplot for Work Type (Private) and Stroke\n",
    "sns.countplot(data=train_data_cleaned, x='work_type_Private', hue='stroke')\n",
    "plt.title('Work Type (Private) vs Stroke')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07dc90b",
   "metadata": {
    "id": "a07dc90b"
   },
   "source": [
    "### Interpretation of Dummy Variables\n",
    "\n",
    "1. **Gender vs Stroke**: We observe that there may be slight differences in stroke prevalence between males and females, although the relationship is not very strong.\n",
    "2. **Smoking Status vs Stroke**: The plot reveals that individuals who have smoked or are currently smoking might be slightly more likely to experience a stroke than those who have never smoked.\n",
    "3. **Work Type vs Stroke**: Some work types, like private and self-employed jobs, may be associated with different stroke rates, but the difference is not very pronounced.\n",
    "\n",
    "These categorical variables may still contribute to stroke prediction in combination with other features during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ddc33",
   "metadata": {
    "id": "864ddc33"
   },
   "source": [
    "## 1.9 Feature Selection\n",
    "\n",
    "Based on the correlation matrix and the analysis of dummy variables, we will now remove features that either show weak correlations with the target variable (`stroke`) or are redundant due to the encoding of categorical variables. This step helps reduce the complexity of our model and ensures that we focus on the most important and independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4942194",
   "metadata": {
    "id": "f4942194",
    "outputId": "5d06d2e3-462f-405a-eb4f-c89cc87d333d"
   },
   "outputs": [],
   "source": [
    "# Dropping redundant dummy variables\n",
    "columns_to_drop = ['Residence_type_Urban', 'ever_married_Yes']  # Adjusted for correct dummy variables\n",
    "train_data_cleaned = train_data_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "# Check remaining features\n",
    "train_data_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f084c",
   "metadata": {
    "id": "ac9f084c"
   },
   "source": [
    "### Interpretation of Feature Selection\n",
    "\n",
    "We have removed some features that were either redundant or had a very weak correlation with the target variable. Removing these features reduces the dimensionality of our dataset and helps prevent overfitting, while focusing on the most impactful variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171858b",
   "metadata": {
    "id": "5171858b"
   },
   "source": [
    "### 1.10 Main Findings of the Exploratory Data Analysis (EDA)\n",
    "\n",
    "From our analysis of the dataset, we can summarize the following key insights:\n",
    "\n",
    "#### 1. **Age**:\n",
    "   - **Distribution**: Age shows a wide range of values from infancy to older adulthood. The majority of individuals are between 20 and 60 years old.\n",
    "   - **Relationship with Stroke**: Individuals who had a stroke tend to be older, as the median age of stroke patients is higher than that of non-stroke patients. The boxplots also reveal that age has a notable impact on the likelihood of a stroke, as older individuals seem to have a higher probability of having experienced a stroke.\n",
    "   \n",
    "#### 2. **BMI (Body Mass Index)**:\n",
    "   - **Distribution**: BMI values vary, with most people having a BMI between 20 and 35. There are a few outliers with very high BMI values.\n",
    "   - **Relationship with Stroke**: While individuals who had a stroke generally have slightly higher BMI values on average, the difference is not as pronounced as with age. The scatterplot and boxplot suggest that BMI alone might not be a strong indicator for predicting strokes, though individuals with very high BMIs might be at increased risk.\n",
    "\n",
    "#### 3. **Average Glucose Level**:\n",
    "   - **Distribution**: Glucose levels show significant variation across individuals. Most people have glucose levels between 70 and 120, but there are some extreme outliers.\n",
    "   - **Relationship with Stroke**: Stroke patients tend to have higher average glucose levels. The boxplot shows a higher median glucose level for individuals who had a stroke, indicating that glucose levels could be an important factor to consider in stroke prediction.\n",
    "\n",
    "#### 4. **Hypertension and Heart Disease**:\n",
    "   - Although we have not visualized these features yet, earlier correlation analysis revealed that hypertension and heart disease have some relationship with stroke. Individuals with hypertension or heart disease are more likely to have a stroke, although the strength of the relationship is moderate.\n",
    "\n",
    "#### 5. **Gender, Smoking, and Work Type**:\n",
    "   - These features were already converted to dummy variables. While the EDA focused on continuous variables, these categorical variables can still be valuable predictors in a machine learning model. Their potential impact on stroke risk will likely emerge during model training and evaluation.\n",
    "\n",
    "#### Conclusion:\n",
    "The exploratory data analysis reveals that certain features such as age, glucose level, and, to a lesser extent, BMI, have a relationship with the likelihood of stroke. Age appears to be the most significant predictor so far, with older individuals having a noticeably higher risk. Hypertension and heart disease also play a role in stroke risk. These findings will guide the feature selection and preprocessing steps for model development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cac5a",
   "metadata": {
    "id": "5b6cac5a"
   },
   "source": [
    "## 1.10 Standardizing the Data\n",
    "\n",
    "### What is Standardization?\n",
    "\n",
    "Standardization is the process of transforming data so that it has a mean of 0 and a standard deviation of 1. This is done by subtracting the mean of a feature from each data point and then dividing by the standard deviation of that feature. The formula is:\n",
    "\n",
    "\\[\n",
    "Z = \\frac{X - \\mu}{\\sigma}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( X \\) is the original data point,\n",
    "- \\( \\mu \\) is the mean of the feature, and\n",
    "- \\( \\sigma \\) is the standard deviation of the feature.\n",
    "\n",
    "This process ensures that all features are on the same scale, which is particularly important for certain machine learning algorithms that depend on the distance between data points.\n",
    "\n",
    "### When is Standardization Needed?\n",
    "\n",
    "Standardization is particularly useful for machine learning algorithms that rely on distance measures, such as:\n",
    "- **Support Vector Machines (SVM)**\n",
    "- **k-Nearest Neighbors (k-NN)**\n",
    "- **Logistic Regression**\n",
    "\n",
    "These algorithms are sensitive to the scale of the input features. If the features are not standardized, variables with larger ranges can dominate the distance calculations, leading to biased model predictions. This issue was discussed in the DataCamp course, particularly when using models like **k-NN** and **SVM**, where distances between points are crucial for performance.\n",
    "\n",
    "For algorithms like **decision trees** and **random forests**, standardization is usually not necessary, as these models are based on splitting the data and are not influenced by the scale of the features.\n",
    "\n",
    "### Does it Add Value to Our Dataset?\n",
    "\n",
    "Based on the findings from our **Exploratory Data Analysis (EDA)**, we observed that the features `age`, `bmi`, and `avg_glucose_level` have different scales. For instance:\n",
    "- **Age** has a wide range from infancy to old age (0 to 82).\n",
    "- **BMI** values range from 10.1 to 97.6, with some extreme outliers.\n",
    "- **Average glucose levels** range from 55.01 mg/dL to 281.59 mg/dL.\n",
    "\n",
    "Given that we plan to use **distance-based models**, such as **k-Nearest Neighbors (k-NN)** and **Logistic Regression**, it is important to standardize these features. Without standardization, features like `avg_glucose_level`, which have higher numerical values, could dominate the distance calculations and skew the results of models like **k-NN**.\n",
    "\n",
    "In contrast, for models like **Random Forests** or **Decision Trees**, which are not affected by feature scale, standardization is not required. However, given that our model selection includes both types of algorithms, we will proceed with standardization to ensure consistency across the models that require it.\n",
    "\n",
    "### Conclusion:\n",
    "Standardization will add value to our dataset, particularly for models like **k-NN**, **SVM**, and **Logistic Regression**, where feature scale impacts performance. Therefore, we will standardize `age`, `bmi`, and `avg_glucose_level` to ensure these features are on the same scale, thereby improving the performance of distance-based algorithms.\n",
    "\n",
    "### Steps:\n",
    "We will now apply standardization to the features `age`, `bmi`, and `avg_glucose_level`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b046784",
   "metadata": {
    "id": "6b046784"
   },
   "outputs": [],
   "source": [
    "# Import necessary library for standardization                        sicher nur die 3??????????????\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the columns to be standardized\n",
    "columns_to_standardize = ['age', 'bmi', 'avg_glucose_level']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler to the relevant columns\n",
    "train_data_cleaned[columns_to_standardize] = scaler.fit_transform(train_data_cleaned[columns_to_standardize])\n",
    "\n",
    "# Check the first few rows to ensure standardization was applied\n",
    "train_data_cleaned[columns_to_standardize].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e533630",
   "metadata": {
    "id": "9e533630"
   },
   "source": [
    "## 1.11 Correlation Matrix After Standardization\n",
    "\n",
    "Now that we have standardized the data, we will compute the correlation matrix again. This helps us understand how the standardized features are related to one another and to the target variable (`stroke`).\n",
    "\n",
    "The following heatmap shows the correlations between the standardized features, which can guide our feature selection and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f54f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "402f54f6",
    "outputId": "3855c00f-03cb-49d1-fbb8-9f12187a7a20"
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix after standardization\n",
    "correlation_matrix = train_data_cleaned.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix After Standardization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e6a0d",
   "metadata": {
    "id": "207e6a0d"
   },
   "source": [
    "The correlation matrix helps us identify how strongly different features are related to one another and to the target variable (`stroke`).\n",
    "\n",
    "### Key Observations:\n",
    "1. **Age**:\n",
    "   - The positive correlation between **`age`** and **`stroke`** (0.15) suggests that older individuals are more likely to experience a stroke. This confirms the trend we observed in the visualizations (histogram and boxplot).\n",
    "   \n",
    "2. **BMI (Body Mass Index)**:\n",
    "   - The correlation between **`BMI`** and **`stroke`** is weak (0.07). While there is a slight positive relationship, **BMI** alone may not be a strong predictor of strokes.\n",
    "   \n",
    "3. **Average Glucose Level**:\n",
    "   - **`avg_glucose_level`** shows a moderate positive correlation with **`stroke`** (0.10). This indicates that individuals with higher glucose levels may be at a higher risk of stroke, which aligns with the insights from the boxplots.\n",
    "\n",
    "4. **Hypertension and Heart Disease**:\n",
    "   - **`hypertension`** (0.08) and **`heart_disease`** (0.06) also show a slight positive correlation with **`stroke`**, suggesting that individuals with high blood pressure or heart disease are somewhat more prone to having a stroke. However, these are not strong indicators, implying that other factors play a significant role as well.\n",
    "   \n",
    "5. **Low Correlations Between Features**:\n",
    "   - Most features do not have strong correlations with each other, indicating that multicollinearity is not a major issue in this dataset. This means each feature likely provides unique information to the model.\n",
    "\n",
    "6. **Other Observations**:\n",
    "   - The variable **`ever_married_Yes`** is strongly correlated with **`age`** (0.70), indicating that older individuals are more likely to be married. However, its direct correlation with **`stroke`** is low (0.07), meaning that marital status alone is not a strong predictor of stroke.\n",
    "\n",
    "### Summary:\n",
    "The variables **`age`**, **`avg_glucose_level`**, and, to a lesser extent, **`BMI`**, show some relationship with **`stroke`** and are likely to be important for model building. Additionally, the lack of high correlations between features suggests that each feature brings unique information to the model, which is beneficial for model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "McJyWQM-8OR_",
   "metadata": {
    "id": "McJyWQM-8OR_"
   },
   "source": [
    "### Heatmap for Correlation Between Independent Variables\n",
    "\n",
    "The heatmap below visualizes the correlation between the independent variables. This helps us detect any potential multicollinearity, where certain features might be highly correlated with each other. Multicollinearity can negatively affect model performance, especially for models like logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OFPmw2jY8Pm1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "OFPmw2jY8Pm1",
    "outputId": "0abfb2da-8a48-49c2-ce24-e5585870c6ee"
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix for independent variables only\n",
    "correlation_matrix = train_data_cleaned.drop(columns=['stroke']).corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix for Independent Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCw-MTXO9A1i",
   "metadata": {
    "id": "yCw-MTXO9A1i"
   },
   "source": [
    "### Interpretation of the Correlation Matrix for Independent Variables\n",
    "\n",
    "1. **Age and ever_married_Yes**:\n",
    "   - There is a strong positive correlation (0.70) between `age` and `ever_married_Yes`, indicating that older individuals are more likely to be married. This could lead to multicollinearity issues in models sensitive to correlated features, such as logistic regression.\n",
    "\n",
    "2. **Age and work_type_children**:\n",
    "   - A strong negative correlation (-0.64) exists between `age` and `work_type_children`, which is expected, as only younger individuals (children) fall into this work category.\n",
    "\n",
    "3. **BMI and age**:\n",
    "   - There is a moderate positive correlation (0.39) between `BMI` and `age`, suggesting that older individuals tend to have higher BMI, though the correlation is not strong enough to raise immediate concerns.\n",
    "\n",
    "In general, there are no extremely high correlations between most independent variables, indicating that multicollinearity may not be a major issue for most features, but `age` and `ever_married_Yes` should be handled with care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9fa68",
   "metadata": {
    "id": "07a9fa68"
   },
   "source": [
    "### 1.12 Outlier Detection\n",
    "\n",
    "After standardizing the data, we will now investigate potential outliers in important variables such as `BMI` and `avg_glucose_level`. Outliers can significantly affect the performance of certain models, so we will visualize these variables to decide whether any extreme values should be removed or adjusted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f79645",
   "metadata": {
    "id": "a4f79645",
    "outputId": "6f2a879e-ea44-46f8-f5ef-ea72596d41f7"
   },
   "outputs": [],
   "source": [
    "# Scatterplot with logistic regression line for BMI vs Stroke\n",
    "sns.lmplot(x='bmi', y='stroke', data=train_data_cleaned, logistic=True, scatter_kws={\"s\": 50, \"alpha\": 0.5})\n",
    "plt.title('BMI vs Stroke with Logistic Regression Line')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot with logistic regression line for Glucose Level vs Stroke\n",
    "sns.lmplot(x='avg_glucose_level', y='stroke', data=train_data_cleaned, logistic=True, scatter_kws={\"s\": 50, \"alpha\": 0.5})\n",
    "plt.title('Glucose Level vs Stroke with Logistic Regression Line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e658b5f9",
   "metadata": {
    "id": "e658b5f9",
    "outputId": "729bffe5-2e17-480a-8a7e-26c5b761c6e7"
   },
   "source": [
    "### Interpretation of Outlier Detection\n",
    "\n",
    "1. **BMI vs Stroke**: We observe that there are some very high BMI values that could be considered outliers. These extreme values may skew the model's performance, and further analysis is needed to decide whether to remove or adjust them.\n",
    "2. **Glucose Level vs Stroke**: Similarly, we notice some very high glucose levels that could be outliers. Extreme glucose levels might indicate an increased risk of stroke, but these values should be carefully evaluated before training the model.\n",
    "\n",
    "Outliers can be problematic for some machine learning models, so we may consider removing or adjusting these extreme values in future preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3OmaW0_d8FVW",
   "metadata": {
    "id": "3OmaW0_d8FVW"
   },
   "source": [
    "### 1.13 Handling Class Imbalance\n",
    "\n",
    "The dataset shows a significant class imbalance, with only around 1.5% of the patients having suffered a stroke. To account for this imbalance, we will use **stratified splitting** when dividing the data into training and test sets. This ensures that the proportion of stroke cases is maintained in both sets.\n",
    "\n",
    "Other techniques like **Oversampling** (e.g., SMOTE) could also be applied to balance the classes during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bp3zmUrr8G8U",
   "metadata": {
    "id": "Bp3zmUrr8G8U"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified splitting to maintain class imbalance proportions in both sets\n",
    "X = train_data_cleaned.drop(columns=['stroke'])\n",
    "y = train_data_cleaned['stroke']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(\"Stroke distribution in the training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d996e1d4",
   "metadata": {
    "id": "d996e1d4",
    "outputId": "494c1973-22b0-4f6d-e309-99c14fd45e70"
   },
   "source": [
    "# Sub-Assignment 2: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SDW4S6TR8YOA",
   "metadata": {
    "id": "SDW4S6TR8YOA"
   },
   "source": [
    "## 2. Model Evaluation: Logistic Regression\n",
    "\n",
    "In addition to the Random Forest model, we will now train and evaluate a **Logistic Regression** model. By comparing multiple models, we can select the best one for predicting strokes in this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pT2Yz2li8anS",
   "metadata": {
    "id": "pT2Yz2li8anS"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ljbGJc5N7JR",
   "metadata": {
    "id": "5ljbGJc5N7JR"
   },
   "source": [
    "## 2. Transition to Modeling\n",
    "\n",
    "After completing the exploratory data analysis (EDA) and cleaning the data, we are now ready to move on to building predictive models.\n",
    "\n",
    "Key insights from the EDA that will inform the modeling process:\n",
    "- Features like age, BMI, and glucose levels show distinct differences between stroke and non-stroke groups.\n",
    "- The data contains a significant class imbalance, which we need to address in the modeling phase.\n",
    "- Outliers were identified in variables such as BMI and were handled during the cleaning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c40f12",
   "metadata": {
    "id": "89c40f12"
   },
   "source": [
    "# Assignment 2: Model Evaluation\n",
    "\n",
    "In this section, we will discuss which evaluation metrics can be used to assess the performance of a machine learning model in the context of stroke prediction. Then, we will apply these metrics using a machine learning algorithm of our choice.\n",
    "\n",
    "## 1. Which Metrics Can Be Used?\n",
    "\n",
    "For binary classification problems like stroke prediction, where the goal is to predict whether or not a person will have a stroke, several evaluation metrics are commonly used:\n",
    "\n",
    "### 1.1 Accuracy\n",
    "- **Definition**: Accuracy is the proportion of correct predictions out of the total number of predictions.\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "  \\]\n",
    "- **When to use**: Accuracy works well when the classes are balanced. However, in cases of class imbalance (like stroke prediction, where most patients do not have a stroke), accuracy might be misleading because it could be high even if the model fails to predict strokes correctly.\n",
    "\n",
    "### 1.2 Precision\n",
    "- **Definition**: Precision is the proportion of positive predictions that are actually correct. In the context of stroke prediction, it tells us how many of the patients predicted to have a stroke actually did have a stroke.\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  \\]\n",
    "- **When to use**: Precision is important when false positives (predicting stroke when it did not happen) are costly.\n",
    "\n",
    "### 1.3 Recall (Sensitivity)\n",
    "- **Definition**: Recall measures how well the model identifies all actual positive cases (people who actually had a stroke).\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  \\]\n",
    "- **When to use**: Recall is important when missing positive cases (like missing stroke patients) is costly.\n",
    "\n",
    "### 1.4 F1 Score\n",
    "- **Definition**: The F1 score is the harmonic mean of Precision and Recall. It provides a balanced measure when both precision and recall are important.\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  \\]\n",
    "- **When to use**: The F1 score is useful when there is an uneven class distribution and you need a balance between precision and recall.\n",
    "\n",
    "### 1.5 AUC-ROC (Area Under the Receiver Operating Characteristic Curve)\n",
    "- **Definition**: The AUC-ROC measures the ability of the model to distinguish between classes. The ROC curve plots the true positive rate (recall) against the false positive rate.\n",
    "- **When to use**: AUC-ROC is useful when you want to evaluate how well the model separates the two classes.\n",
    "\n",
    "## 2. Applying the Metrics with a Machine Learning Algorithm\n",
    "\n",
    "We will now train a simple machine learning model (e.g., a Random Forest) and evaluate it using these metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f01bc115",
   "metadata": {
    "id": "f01bc115"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = train_data_cleaned.drop(columns=['stroke'])\n",
    "y = train_data_cleaned['stroke']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31349f",
   "metadata": {
    "id": "8e31349f"
   },
   "outputs": [],
   "source": [
    "# Train a simple Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241272b",
   "metadata": {},
   "source": [
    "## 2.1 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54fb7f7",
   "metadata": {},
   "source": [
    "Knn is a supervised algorithm, that can classify a data-point by evaluating which value of the feature of the k nearest data-points is present most often.\n",
    "There are a few possible ways to measure the distance between the data-points (https://www.ibm.com/topics/knn), but the Euclidean Distance and the Manhattan Distance are the most popular ones:\n",
    "- Euclidean Distance:\n",
    "- Manhattan Distance:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40e6c7",
   "metadata": {},
   "source": [
    "First the data is split inot a training- and a test-set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3d8a8",
   "metadata": {},
   "source": [
    "Then the data is standardized, so both variables are weighted the same: ???????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35df632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the variables           done above but only for a few?\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf872c11",
   "metadata": {},
   "source": [
    "To find the best k, we use the cross-validation on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_values = [i for i in range (1,31)] # https://www.datacamp.com/de/tutorial/k-nearest-neighbor-classification-scikit-learn\n",
    "scores = []\n",
    "\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "    scores.append(np.mean(score))\n",
    "\n",
    "print(max(scores))\n",
    "print(scores.index(max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = k_values, y = scores, marker = 'o') # https://www.datacamp.com/de/tutorial/k-nearest-neighbor-classification-scikit-learn\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Accuracy Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "precision = precision_score(y_test, y_pred_knn)\n",
    "recall = recall_score(y_test, y_pred_knn)\n",
    "f1 = f1_score(y_test, y_pred_knn)\n",
    "roc_auc = roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Display the results                                                            which to choose?\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576fee5",
   "metadata": {},
   "source": [
    "Explain how regularisation works for this model??????????? Test size 0.2?????????????????"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
