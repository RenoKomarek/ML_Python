{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ad4110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k-value: {'n_neighbors': 1}\n",
      "Best cross-validation f1-score: 0.05698086468104273\n",
      "Test Accuracy: 0.9703427719821163\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      6600\n",
      "           1       0.03      0.03      0.03       110\n",
      "\n",
      "    accuracy                           0.97      6710\n",
      "   macro avg       0.51      0.51      0.51      6710\n",
      "weighted avg       0.97      0.97      0.97      6710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing the needed libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#loading the training dataframe\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "#determining the feature-values and target-value\n",
    "X = data.drop(\"stroke\", axis=1)\n",
    "y = data[\"stroke\"]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#initiating KNN and using using gridsearch to find the right k-value for the best f1-score\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': range(1, 15)}\n",
    "stratified_kfold = StratifiedKFold(n_splits=5)\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=stratified_kfold, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print parameters and best score found by GridSearchCV\n",
    "print(\"Best k-value:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation f1-score:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "# Show the model performance\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec2e41",
   "metadata": {},
   "source": [
    "The K_Nearest_neighbors (kNN) model is a model that uses distances to make predictions. Those are the distances between the data points. These distances can be calculated in several ways. 2 of those are the Euclidean distance and the Manhattan distance.\n",
    "\n",
    "\n",
    "### Euclidean Distance:\n",
    "\n",
    "$$d(x, y) = \\sqrt{(x_1 - x_2) + (y_1 - y_2)^2}$$\n",
    "\n",
    "For the Euclidean distance the length of the straight line between 2 data points is calculated. \n",
    "\n",
    "\n",
    "### Manhattan Distance:\n",
    "\n",
    "$$d(x, y) =|x_1 - x_2| + |y_1 - y_2|$$\n",
    "\n",
    "This is the sum of the absolute differences between the coordinates of two points.\n",
    "\n",
    "As the distances are calculated, for each point it can be determined what other points are closest. If you want to classify the category of a certain point, letâ€™s call it point 1, then the model looks at the k amount of closest points and determines the modus of the categories of these points. The modus will be the category of point 1. This is done for all the point that have to be classified. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1634698",
   "metadata": {},
   "source": [
    "### Standardization for kNN\n",
    "Standardization is of great importance for this model. kNN depends on calculating distances between data points. The distances metrics Euclidean distance and Manhattan distance are highly influenced by the scale of  each feature. For example, if on feature would be in much larger scale than the others, it will have a greater influence on the model than the other features. To prevent this, standardization is a good solution. It makes sure all the features are the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d93610",
   "metadata": {},
   "source": [
    "### Regularization for kNN\n",
    "For kNN the choice for the value of k can be seen as a form of regularization. Choosing a smaller k will make the model more sensitive to noise. Noise in data means random or unwanted variation in the data which doesn't add to the classifcation of the model. This means, when choosen a smal value for k, the model reacts strongly to individual points, eventhough they might be outliers, which could lead to overfitting.\n",
    "Choosing a higher value for k, can help reduce the effect of the noise. On the other hand, if k is too large, the model can be too averaged, which could lead to underfitting.\n",
    "By selecting the optimal k value, it is possible finding the right balance between over- and underfitting and in that way regularizing the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
